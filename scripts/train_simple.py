#!/usr/bin/env python3
"""
ğŸŒ¸ Plant Classifier - Basit EÄŸitim Scripti
Flowers102 + MobileNetV2
"""

import tensorflow as tf
import tensorflow_datasets as tfds
import numpy as np
from pathlib import Path
import sys

print("=" * 50)
print("ğŸŒ¸ Plant Classifier EÄŸitimi")
print("=" * 50)

print("\nğŸ“¥ Dataset indiriliyor...")
(train_ds, val_ds, test_ds), metadata = tfds.load(
    'oxford_flowers102',
    split=['train', 'validation', 'test'],
    with_info=True,
    as_supervised=True
)

NUM_CLASSES = metadata.features['label'].num_classes
print(f"âœ… Dataset yÃ¼klendi: {NUM_CLASSES} sÄ±nÄ±f")
print(f"   - EÄŸitim: {len(train_ds)} Ã¶rnek")
print(f"   - Validation: {len(val_ds)} Ã¶rnek")
print(f"   - Test: {len(test_ds)} Ã¶rnek")

print("\nğŸ”„ Veri iÅŸleniyor...")
IMG_SIZE = 224
BATCH_SIZE = 32

def preprocess(image, label):
    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])
    image = tf.keras.applications.mobilenet_v2.preprocess_input(image)
    return image, label

def augment(image, label):
    # GÃ¼Ã§lÃ¼ augmentation - %85 iÃ§in gerekli
    image = tf.image.resize(image, [IMG_SIZE + 40, IMG_SIZE + 40])
    image = tf.image.random_crop(image, [IMG_SIZE, IMG_SIZE, 3])
    image = tf.image.random_flip_left_right(image)
    # Renk augmentasyonlarÄ±
    image = tf.image.random_brightness(image, 0.3)
    image = tf.image.random_contrast(image, 0.7, 1.3)
    image = tf.image.random_saturation(image, 0.7, 1.3)
    image = tf.image.random_hue(image, 0.1)
    # Random erase (cutout benzeri)
    if tf.random.uniform([]) > 0.5:
        h, w = IMG_SIZE // 4, IMG_SIZE // 4
        y = tf.random.uniform([], 0, IMG_SIZE - h, dtype=tf.int32)
        x = tf.random.uniform([], 0, IMG_SIZE - w, dtype=tf.int32)
        mask = tf.ones([h, w, 3])
        mask = tf.pad(mask, [[y, IMG_SIZE - h - y], [x, IMG_SIZE - w - x], [0, 0]])
        image = image * (1 - mask) + mask * tf.random.uniform([1, 1, 3], -1, 1)
    image = tf.keras.applications.mobilenet_v2.preprocess_input(image)
    return image, label

train_ds = train_ds.map(augment, num_parallel_calls=tf.data.AUTOTUNE)
train_ds = train_ds.shuffle(1000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

val_ds = val_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)
val_ds = val_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

test_ds = test_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)
test_ds = test_ds.batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)

print("âœ… Dataset hazÄ±r!")

print("\nğŸ§  Model oluÅŸturuluyor...")
base_model = tf.keras.applications.MobileNetV2(
    input_shape=(IMG_SIZE, IMG_SIZE, 3),
    include_top=False,
    weights='imagenet'
)
base_model.trainable = False

model = tf.keras.Sequential([
    base_model,
    tf.keras.layers.GlobalAveragePooling2D(),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(768, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.5),
    tf.keras.layers.Dense(512, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.0001)),
    tf.keras.layers.BatchNormalization(),
    tf.keras.layers.Dropout(0.4),
    tf.keras.layers.Dense(NUM_CLASSES, activation='softmax')
])

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

print("âœ… Model hazÄ±r!")
print(f"   - Toplam parametre: {model.count_params():,}")

print("\nğŸš€ EÄŸitim baÅŸlÄ±yor...")
print("=" * 50)

# Cosine decay learning rate
lr_schedule = tf.keras.optimizers.schedules.CosineDecay(
    initial_learning_rate=0.001,
    decay_steps=30 * len(train_ds),
    alpha=0.0001
)

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

callbacks = [
    tf.keras.callbacks.EarlyStopping(
        monitor='val_accuracy',
        patience=10,
        restore_best_weights=True,
        mode='max'
    )
]

print("ğŸ“ AÅŸama 1: Transfer Learning (Ã¼st katmanlarÄ± eÄŸit - 30 epoch)")
history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=30,
    callbacks=callbacks,
    verbose=1
)

# Fine-tuning AÅŸama 1: Base model'in yarÄ±sÄ±nÄ± aÃ§
print("\nğŸ“ AÅŸama 2: Fine-tuning (base model'in yarÄ±sÄ± - 20 epoch)")
base_model.trainable = True

# Ä°lk yarÄ± dondur, ikinci yarÄ± eÄŸit
for layer in base_model.layers[:len(base_model.layers)//2]:
    layer.trainable = False

print(f"   - EÄŸitilebilir base katman: {sum([1 for l in base_model.layers if l.trainable])}")

lr_schedule_fine1 = tf.keras.optimizers.schedules.CosineDecay(
    initial_learning_rate=0.0001,
    decay_steps=20 * len(train_ds),
    alpha=0.00001
)

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule_fine1),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

history_fine1 = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=20,
    callbacks=callbacks,
    verbose=1
)

# Fine-tuning AÅŸama 2: TÃ¼m base model'i aÃ§
print("\nğŸ“ AÅŸama 3: Full Fine-tuning (tÃ¼m model - 15 epoch)")
for layer in base_model.layers:
    layer.trainable = True

print(f"   - EÄŸitilebilir toplam katman: {len(model.trainable_variables)}")

lr_schedule_fine2 = tf.keras.optimizers.schedules.CosineDecay(
    initial_learning_rate=0.00003,
    decay_steps=15 * len(train_ds),
    alpha=0.000001
)

model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=lr_schedule_fine2),
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

history_fine2 = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=15,
    callbacks=callbacks,
    verbose=1
)

print("\nğŸ“Š Test ediliyor...")
test_loss, test_accuracy = model.evaluate(test_ds, verbose=0)
print(f"âœ… Test Accuracy: {test_accuracy * 100:.2f}%")


print("\nğŸ’¾ Model kaydediliyor...")
output_dir = Path(__file__).parent.parent / 'output'
output_dir.mkdir(exist_ok=True)

model_path = output_dir / 'plant_classifier.keras'
model.save(model_path)
print(f"âœ… Model kaydedildi: {model_path}")

# CoreML'e dÃ¶nÃ¼ÅŸtÃ¼r
print("\nğŸ”„ CoreML'e dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼yor...")
try:
    import coremltools as ct
    
    # Label isimlerini yÃ¼kle
    labels_path = Path(__file__).parent.parent / 'labels.txt'
    with open(labels_path, 'r') as f:
        class_labels = [line.strip() for line in f.readlines() if line.strip()]
    
    print(f"   - {len(class_labels)} sÄ±nÄ±f label'Ä± yÃ¼klendi")
    
    # Model input ismini al
    input_name = model.input.name.split(':')[0]
    print(f"   - Model input ismi: {input_name}")
    
    # CoreML'e dÃ¶nÃ¼ÅŸtÃ¼r
    coreml_model = ct.convert(
        model,
        inputs=[
            ct.ImageType(
                name=input_name,
                shape=(1, 224, 224, 3),
                scale=1/127.5,  # MobileNetV2 preprocessing
                bias=[-1, -1, -1],
                color_layout=ct.colorlayout.RGB
            )
        ],
        classifier_config=ct.ClassifierConfig(class_labels)
    )
    
    # Metadata ekle
    coreml_model.author = "Plant Classifier"
    coreml_model.short_description = "Flowers102 dataset ile eÄŸitilmiÅŸ Ã§iÃ§ek sÄ±nÄ±flandÄ±rÄ±cÄ± (102 sÄ±nÄ±f)"
    coreml_model.version = "1.0"
    
    # Kaydet
    coreml_path = output_dir / 'PlantClassifier.mlpackage'
    coreml_model.save(str(coreml_path))
    print(f"âœ… CoreML model kaydedildi: {coreml_path}")
    
except Exception as e:
    print(f"âš ï¸  CoreML dÃ¶nÃ¼ÅŸÃ¼mÃ¼ baÅŸarÄ±sÄ±z: {e}")
    print("   Keras model kaydedildi, CoreML'e manuel dÃ¶nÃ¼ÅŸtÃ¼rebilirsiniz.")
    print("   Manuel dÃ¶nÃ¼ÅŸtÃ¼rme: python scripts/convert_to_coreml.py")

print("\n" + "=" * 50)
print("ğŸ‰ EÄÄ°TÄ°M TAMAMLANDI!")
print("=" * 50)
print(f"\nFinal Test Accuracy: {test_accuracy * 100:.2f}%")
print(f"Model: {model_path}")